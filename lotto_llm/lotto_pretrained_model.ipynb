{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1035248d",
   "metadata": {},
   "source": [
    "# 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bd9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자수 : 72\n",
      "토큰수 : 15\n",
      "[220, 543, 5850, 7342, 6129, 1088, 262, 2119, 24433, 339, 991, 550, 465, 1336, 82]\n",
      "  which Harry watched fly around the room wishing he still had his fulls\n",
      "220 :  \n",
      "543 :  which\n",
      "5850 :  Harry\n",
      "7342 :  watched\n",
      "6129 :  fly\n",
      "1088 :  around\n",
      "262 :  the\n",
      "2119 :  room\n",
      "24433 :  wishing\n",
      "339 :  he\n",
      "991 :  still\n",
      "550 :  had\n",
      "465 :  his\n",
      "1336 :  full\n",
      "82 : s\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"  which Harry watched fly around the room wishing he still had his fulls\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"글자수 :\", len(text))  # 글자수: 26\n",
    "print(\"토큰수 :\", len(tokens))  # 토큰수: 6\n",
    "print(tokens)  # [15496, 2159, 257, 281, 3453, 13]\n",
    "print(tokenizer.decode(tokens))  # Harry Potter was a wizard.\n",
    "for token in tokens:\n",
    "    print(token, \":\", tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e8c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\github\\llm\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 50257\n",
      "pad_token: <|endoftext|> id: 50256\n",
      "eos_token: <|endoftext|> id: 50256\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. GPT-2 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 2. GPT-2는 기본 pad_token이 없어서 직접 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"vocab_size:\", tokenizer.vocab_size)\n",
    "print(\"pad_token:\", tokenizer.pad_token, \"id:\", tokenizer.pad_token_id)\n",
    "print(\"eos_token:\", tokenizer.eos_token, \"id:\", tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d61185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 개수: 10000\n",
      "원본 샘플:\n",
      "money=4000\n",
      "winning=1,11,15,18,29,38\n",
      "bonus=33\n",
      "###\n",
      "티켓수=4\n",
      "구매번호:\n",
      "[2,7,9,15,16,18]\n",
      "[3,6,28,35,38,44]\n",
      "[2,6,14,15,33,39]\n",
      "[2,13,27,35,36,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치=0\n",
      "수익률=0.0%\n"
     ]
    }
   ],
   "source": [
    "def load_text_samples(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().strip()\n",
    "    # 샘플 사이에 빈 줄 하나 넣어놨으니까 \"\\n\\n\" 기준으로 자름\n",
    "    samples = [s.strip() for s in data.split(\"\\n\\n\") if s.strip()]\n",
    "    return samples\n",
    "\n",
    "train_samples = load_text_samples(\"lotto_train.txt\")\n",
    "print(\"샘플 개수:\", len(train_samples))\n",
    "\n",
    "example = train_samples[0]\n",
    "print(\"원본 샘플:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e9b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([1, 256])\n",
      "attention_mask shape: torch.Size([1, 256])\n",
      "디코딩된 텍스트:\n",
      "money=4000\n",
      "winning=1,11,15,18,29,38\n",
      "bonus=33\n",
      "###\n",
      "티켓수=4\n",
      "구매번호:\n",
      "[2,7,9,15,16,18]\n",
      "[3,6,28,35,38,44]\n",
      "[2,6,14,15,33,39]\n",
      "[2,13,27,35,36,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치=0\n",
      "수익률=0.0%\n"
     ]
    }
   ],
   "source": [
    "max_len = 256  # 임시\n",
    "\n",
    "enc = tokenizer(\n",
    "    example,\n",
    "    max_length=max_len,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",  # PyTorch 텐서로\n",
    ")\n",
    "\n",
    "input_ids = enc[\"input_ids\"]        # shape: (1, max_len)\n",
    "attention_mask = enc[\"attention_mask\"]  # shape: (1, max_len)\n",
    "\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "\n",
    "# 디코딩해서 잘 복원되는지 확인\n",
    "decoded = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(\"디코딩된 텍스트:\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class LottoDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.eos = tokenizer.eos_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_txt = self.texts[idx]\n",
    "\n",
    "        txt = raw_txt + self.eos\n",
    "\n",
    "        # max_len+1 길이로 토큰화 → x:[:-1], y:[1:] 사용\n",
    "        enc = self.tokenizer(\n",
    "            txt,\n",
    "            max_length=self.max_len + 1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = enc[\"input_ids\"][0]        # (max_len+1,)\n",
    "        attn_mask = enc[\"attention_mask\"][0]  # (max_len+1,)\n",
    "\n",
    "        # 언어모델용 input/target\n",
    "        x = ids[:-1]        # (max_len,)\n",
    "        y = ids[1:]         # (max_len,)\n",
    "        x_mask = attn_mask[:-1]\n",
    "\n",
    "        return x, y, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e2f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([256])\n",
      "y shape: torch.Size([256])\n",
      "x_mask shape: torch.Size([256])\n",
      "x[:20]: tensor([26316,    28, 27559,   198, 14463,    28,    16,    11,  1157,    11,\n",
      "         1314,    11,  1507,    11,  1959,    11,  2548,   198,  4189,   385])\n",
      "y[:20]: tensor([   28, 27559,   198, 14463,    28,    16,    11,  1157,    11,  1314,\n",
      "           11,  1507,    11,  1959,    11,  2548,   198,  4189,   385,    28])\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "train_ds = LottoDataset(train_samples, tokenizer, max_len=max_len)\n",
    "\n",
    "x, y, x_mask = train_ds[0]\n",
    "\n",
    "print(\"x shape:\", x.shape)          # torch.Size([256])\n",
    "print(\"y shape:\", y.shape)          # torch.Size([256])\n",
    "print(\"x_mask shape:\", x_mask.shape)\n",
    "\n",
    "print(\"x[:20]:\", x[:20])\n",
    "print(\"y[:20]:\", y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5f8c7",
   "metadata": {},
   "source": [
    "DataSet 출력값 예시\n",
    "\n",
    "X TEXT:\n",
    "money=8000\n",
    "winning=1,2,3,4,5,6\n",
    "bonus=7\n",
    "###\n",
    "\n",
    "Y TEXT:\n",
    "oney=8000\n",
    "winning=1,2,3,4,5,6\n",
    "bonus=7\n",
    "###\n",
    "티"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f7d3f",
   "metadata": {},
   "source": [
    "# 전처리 중간정리\n",
    "1.\tGPT-2 토크나이저 선택\n",
    "2.\tAutoTokenizer.from_pretrained(\"gpt2\")로 로딩\n",
    "3.\tpad_token 설정 (eos 재사용)\n",
    "4.\tlotto_train.txt에서 샘플 읽음\n",
    "5.\t토크나이저 encode/decode 테스트\n",
    "6.\t그걸 쓰는 LottoDataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a468413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|>\n",
      "pad_token_id: 50256\n",
      "vocab_size: 50257\n",
      "\n",
      "input_ids shape: torch.Size([256])\n",
      "attention_mask shape: torch.Size([256])\n",
      "\n",
      "=== DECODED TEXT ===\n",
      "money=4000\n",
      "winning=1,11,15,18,29,38\n",
      "bonus=33\n",
      "###\n",
      "티켓수=4\n",
      "구매번호:\n",
      "[2,7,9,15,16,18]\n",
      "[3,6,28,35,38,44]\n",
      "[2,6,14,15,33,39]\n",
      "[2,13,27,35,36,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치=0\n",
      "수익률=0.0%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# GPT-2 tokenizer 준비\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"vocab_size:\", tokenizer.vocab_size)\n",
    "\n",
    "\n",
    "# lotto_train.txt 읽기\n",
    "def load_text_samples(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().strip()\n",
    "    samples = [s.strip() for s in data.split(\"\\n\\n\") if s.strip()]\n",
    "    return samples\n",
    "\n",
    "train_samples = load_text_samples(\"lotto_train.txt\")\n",
    "\n",
    "# 샘플 하나\n",
    "example = train_samples[0]\n",
    "# print(\"=== ORIGINAL TEXT ===\")\n",
    "# print(example)\n",
    "\n",
    "# encode\n",
    "enc = tokenizer(\n",
    "    example,\n",
    "    max_length=256,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = enc[\"input_ids\"][0]\n",
    "attention_mask = enc[\"attention_mask\"][0]\n",
    "\n",
    "print(\"\\ninput_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "\n",
    "# decode\n",
    "decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== DECODED TEXT ===\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4947dc",
   "metadata": {},
   "source": [
    "# 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ae71b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 샘플 수: 10000\n",
      "첫 샘플 원본:\n",
      "money=4000\n",
      "winning=1,11,15,18,29,38\n",
      "bonus=33\n",
      "###\n",
      "티켓수=4\n",
      "구매번호:\n",
      "[2,7,9,15,16,18]\n",
      "[3,6,28,35,38,44]\n",
      "[2,6,14,15,33,39]\n",
      "[2,13,27,35,36,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치=0\n",
      "수익률=0.0%\n"
     ]
    }
   ],
   "source": [
    "def load_text_samples(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().strip()\n",
    "    # 샘플 사이에 빈 줄 하나씩 있다고 가정 → \"\\n\\n\" 기준 split\n",
    "    samples = [s.strip() for s in data.split(\"\\n\\n\") if s.strip()]\n",
    "    return samples\n",
    "\n",
    "train_texts = load_text_samples(\"lotto_train.txt\")\n",
    "print(\"train 샘플 수:\", len(train_texts))\n",
    "print(\"첫 샘플 원본:\")\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8db334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: torch.Size([4, 256])\n",
      "y_batch shape: torch.Size([4, 256])\n",
      "mask_batch shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_len = 256  # 일단 256 정도로 가정\n",
    "batch_size = 4\n",
    "\n",
    "train_ds = LottoDataset(train_texts, tokenizer, max_len=max_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 배치 하나만 꺼내서 확인해보자\n",
    "x_batch, y_batch, mask_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"x_batch shape:\", x_batch.shape)        # (B, T) = (4, 256)\n",
    "print(\"y_batch shape:\", y_batch.shape)        # (4, 256)\n",
    "print(\"mask_batch shape:\", mask_batch.shape)  # (4, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b310f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== X TEXT (모델 입력) =====\n",
      "money=10000\n",
      "winning=5,15,22,25,35,42\n",
      "bonus=21\n",
      "###\n",
      "티켓수=10\n",
      "구매번호:\n",
      "[3,15,30,32,41,42]\n",
      "[10,16,24,27,37,39]\n",
      "[17,20,24,27,35,44]\n",
      "[1,2,5,23,27,36]\n",
      "[11,13,30,33,35,37]\n",
      "[1,13,16,20,22,30]\n",
      "[4,10,21,26,39,41]\n",
      "[2,7,17,26,38,41]\n",
      "[4,10,22,24,28,36]\n",
      "[1,3,25,36,40,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일�\n",
      "\n",
      "===== Y TEXT (정답 타깃) =====\n",
      "=10000\n",
      "winning=5,15,22,25,35,42\n",
      "bonus=21\n",
      "###\n",
      "티켓수=10\n",
      "구매번호:\n",
      "[3,15,30,32,41,42]\n",
      "[10,16,24,27,37,39]\n",
      "[17,20,24,27,35,44]\n",
      "[1,2,5,23,27,36]\n",
      "[11,13,30,33,35,37]\n",
      "[1,13,16,20,22,30]\n",
      "[4,10,21,26,39,41]\n",
      "[2,7,17,26,38,41]\n",
      "[4,10,22,24,28,36]\n",
      "[1,3,25,36,40,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치\n"
     ]
    }
   ],
   "source": [
    "# 배치에서 첫 번째 샘플만 보자\n",
    "x = x_batch[0]        # (T,)\n",
    "y = y_batch[0]        # (T,)\n",
    "mask = mask_batch[0]  # (T,)\n",
    "\n",
    "# 텐서를 리스트로 변환\n",
    "x_ids = x.tolist()\n",
    "y_ids = y.tolist()\n",
    "\n",
    "# 텍스트로 복원\n",
    "x_text = tokenizer.decode(x_ids, skip_special_tokens=True)\n",
    "y_text = tokenizer.decode(y_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"===== X TEXT (모델 입력) =====\")\n",
    "print(x_text)\n",
    "\n",
    "print(\"\\n===== Y TEXT (정답 타깃) =====\")\n",
    "print(y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43da2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class GPTConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_layer: int = 4,\n",
    "        n_head: int = 4,\n",
    "        d_model: int = 256,\n",
    "        d_ff: int = 1024,\n",
    "        max_len: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        pad_id: int = 0,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "        self.pad_id = pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bac05669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.d_model = config.d_model\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model)\n",
    "        self.proj = nn.Linear(config.d_model, config.d_model)\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.max_len, config.max_len))\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            mask.view(1, 1, config.max_len, config.max_len)\n",
    "        )\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.size()\n",
    "        H = self.n_head\n",
    "        head_dim = C // H\n",
    "\n",
    "        qkv = self.qkv(x)              # (B, T, 3C)\n",
    "        q, k, v = qkv.split(C, dim=2)  # (B, T, C) each\n",
    "\n",
    "        q = q.view(B, T, H, head_dim).transpose(1, 2)  # (B, H, T, head_dim)\n",
    "        k = k.view(B, T, H, head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, H, head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5)  # (B, H, T, T)\n",
    "\n",
    "        causal_mask = self.causal_mask[:, :, :T, :T]\n",
    "        att = att.masked_fill(causal_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: (B, T) → (B, 1, 1, T)\n",
    "            pad_mask = attn_mask.view(B, 1, 1, T)\n",
    "            att = att.masked_fill(pad_mask == 0, float(\"-inf\"))\n",
    "\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        y = att @ v                    # (B, H, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fea54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_ff, config.d_model),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.attn(self.ln1(x), attn_mask=attn_mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26fea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding(config.max_len, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # (선택) 입력 임베딩과 출력 head weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "\n",
    "    def forward(self, idx, attn_mask=None):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.size()\n",
    "        device = idx.device\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        pos = pos.unsqueeze(0).expand(B, T)  # (B, T)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28c80492",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = 200, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    max_len = model.config.max_len\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    x = enc[\"input_ids\"].to(device)  # (1, T)\n",
    "    attn_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(x, attn_mask=attn_mask)        # (1, T, vocab)\n",
    "        last_logits = logits[:, -1, :]                # (1, vocab)\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "\n",
    "        # 뒤에 토큰 붙이고, 너무 길어지면 앞에서 잘라냄\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "        attn_mask = torch.cat(\n",
    "            [attn_mask, torch.ones_like(next_id, device=device)], dim=1\n",
    "        )\n",
    "\n",
    "        if x.size(1) > max_len:\n",
    "            x = x[:, -max_len:]\n",
    "            attn_mask = attn_mask[:, -max_len:]\n",
    "\n",
    "    out_ids = x[0].tolist()\n",
    "    text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de740e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "train samples: 10000\n",
      "val samples: 1000\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# 2) 데이터 로딩\n",
    "train_texts = load_text_samples(\"lotto_train.txt\")\n",
    "val_texts = load_text_samples(\"lotto_val.txt\")  # 없다면 주석 처리하고 train만 써도 됨\n",
    "\n",
    "print(\"train samples:\", len(train_texts))\n",
    "print(\"val samples:\", len(val_texts))\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "train_ds = LottoDataset(train_texts, tokenizer, max_len=max_len)\n",
    "val_ds = LottoDataset(val_texts, tokenizer, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# 3) 모델 & optimizer & loss\n",
    "config = GPTConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    d_model=256,\n",
    "    d_ff=1024,\n",
    "    max_len=max_len,\n",
    "    dropout=0.1,\n",
    "    pad_id=pad_id,\n",
    ")\n",
    "\n",
    "model = MiniGPT(config).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab8a79",
   "metadata": {},
   "source": [
    "#학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86353490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] train_loss=1.3327, val_loss=0.9760\n",
      "[Epoch 002] train_loss=1.1994, val_loss=0.9647\n",
      "[Epoch 003] train_loss=1.0896, val_loss=0.9421\n",
      "[Epoch 004] train_loss=1.0263, val_loss=0.9360\n",
      "[Epoch 005] train_loss=0.9879, val_loss=0.9191\n",
      "[Epoch 006] train_loss=0.9570, val_loss=0.8993\n",
      "[Epoch 007] train_loss=0.9278, val_loss=0.8709\n",
      "[Epoch 008] train_loss=0.8958, val_loss=0.8439\n",
      "[Epoch 009] train_loss=0.8700, val_loss=0.8283\n",
      "[Epoch 010] train_loss=0.8524, val_loss=0.8137\n",
      "[Epoch 011] train_loss=0.8398, val_loss=0.8071\n",
      "[Epoch 012] train_loss=0.8284, val_loss=0.7964\n",
      "[Epoch 013] train_loss=0.8213, val_loss=0.7900\n",
      "[Epoch 014] train_loss=0.8155, val_loss=0.7891\n",
      "[Epoch 015] train_loss=0.8112, val_loss=0.7873\n",
      "[Epoch 016] train_loss=0.8073, val_loss=0.7881\n",
      "[Epoch 017] train_loss=0.8033, val_loss=0.7855\n",
      "[Epoch 018] train_loss=0.7996, val_loss=0.7828\n",
      "[Epoch 019] train_loss=0.7958, val_loss=0.7838\n",
      "[Epoch 020] train_loss=0.7933, val_loss=0.7831\n",
      "[Epoch 021] train_loss=0.7911, val_loss=0.7814\n",
      "[Epoch 022] train_loss=0.7891, val_loss=0.7802\n",
      "[Epoch 023] train_loss=0.7876, val_loss=0.7830\n",
      "[Epoch 024] train_loss=0.7861, val_loss=0.7811\n",
      "[Epoch 025] train_loss=0.7847, val_loss=0.7809\n",
      "[Epoch 026] train_loss=0.7835, val_loss=0.7804\n",
      "[Epoch 027] train_loss=0.7824, val_loss=0.7780\n",
      "[Epoch 028] train_loss=0.7817, val_loss=0.7795\n",
      "[Epoch 029] train_loss=0.7805, val_loss=0.7783\n",
      "[Epoch 030] train_loss=0.7798, val_loss=0.7781\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  # ← epoch 크게 잡고 싶다 했으니까 예시로 30으로 설정\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ----- Train -----\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y, mask in train_loader:\n",
    "        x = x.to(device)       # (B, T)\n",
    "        y = y.to(device)       # (B, T)\n",
    "        mask = mask.to(device) # (B, T)\n",
    "\n",
    "        logits = model(x, attn_mask=mask)  # (B, T, vocab)\n",
    "        \n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),  # (B*T, vocab)\n",
    "            y.view(-1)                         # (B*T)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # ----- Validation -----\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            logits = model(x, attn_mask=mask)\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1)\n",
    "            )\n",
    "            val_loss_sum += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss_sum / len(val_loader)\n",
    "\n",
    "    # ----- Epoch 결과 출력 -----\n",
    "    print(f\"[Epoch {epoch:03d}] train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "    # ----- 모델 저장 -----\n",
    "    # 1) 매 epoch마다 체크포인트 저장\n",
    "    ckpt_path = f\"lotto_gpt_epoch{epoch:03d}.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "    # 2) best val loss 갱신 시 별도 저장\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"lotto_gpt_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd90f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = 200, device=\"cpu\"):\n",
    "    \n",
    "    model.eval()\n",
    "    max_len = model.config.max_len\n",
    "\n",
    "    # 1) 처음에는 패딩 없이 실제 길이만큼만 인코딩\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    x = enc[\"input_ids\"].to(device)      # (1, T0)\n",
    "    attn_mask = enc[\"attention_mask\"].to(device)  # (1, T0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 2) 모델에 넣기 전에 길이가 max_len을 넘으면 뒤에서 max_len만 유지\n",
    "        if x.size(1) > max_len:\n",
    "            x = x[:, -max_len:]\n",
    "            attn_mask = attn_mask[:, -max_len:]\n",
    "\n",
    "        # 3) forward\n",
    "        logits = model(x, attn_mask=attn_mask)        # (1, T, vocab)\n",
    "        last_logits = logits[:, -1, :]                # (1, vocab)\n",
    "\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "\n",
    "        # 4) 새 토큰 이어붙이기\n",
    "        x = torch.cat([x, next_id], dim=1)  # (1, T+1)\n",
    "        next_mask = torch.ones_like(next_id, device=device)\n",
    "        attn_mask = torch.cat([attn_mask, next_mask], dim=1)\n",
    "\n",
    "    # 5) 결과 디코딩\n",
    "    out_ids = x[0].tolist()\n",
    "    text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219f7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "checkpoint loaded from: lotto_gpt_best.pt\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "max_len = 256  # 학습 때 사용한 값과 동일하게\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    d_model=256,\n",
    "    d_ff=1024,\n",
    "    max_len=max_len,\n",
    "    dropout=0.1,\n",
    "    pad_id=pad_id,\n",
    ")\n",
    "model = MiniGPT(config).to(device)\n",
    "\n",
    "# 체크포인트 로드 (파일 이름 맞게 수정 가능)\n",
    "ckpt_path = \"lotto_gpt_best.pt\"   # 또는 \"lotto_gpt_epoch030.pt\" 등\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "model.eval()\n",
    "print(\"checkpoint loaded from:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88c3e40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = 200, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    max_len = model.config.max_len\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    x = enc[\"input_ids\"].to(device)  # (1, T0)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if x.size(1) > max_len:\n",
    "            x = x[:, -max_len:]\n",
    "\n",
    "        logits = model(x)               # (1, T, vocab)\n",
    "        last_logits = logits[:, -1, :]  # (1, vocab)\n",
    "\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_token = next_id.item()\n",
    "\n",
    "        # 1) EOS 나오면 바로 멈추기\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "\n",
    "    out_ids = x[0].tolist()\n",
    "    text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1ba4f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt len (tokens): torch.Size([1, 25])\n",
      "model max_len: 256\n",
      "=== SAMPLE GENERATION ===\n",
      "money=5000\n",
      "winning=1,2,3,4,5,6\n",
      "bonus=7\n",
      "###\n",
      "티켓수=5\n",
      "구매번호:\n",
      "[13,17,24,20,26,33]\n",
      "[1,2,3,8,27,29]\n",
      "[5,21,29,30,31,36]\n",
      "[12,19,19,20,33,42]\n",
      "[5,6,16,21,24,42]\n",
      "3개일치=0\n",
      "4개일치=0\n",
      "5개일치=0\n",
      "5개보너스일치=0\n",
      "6개일치=0\n",
      "수익률=0.0%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    }
   ],
   "source": [
    "example_prompt = (\n",
    "    \"money=5000\\n\"\n",
    "    \"winning=1,2,3,4,5,6\\n\"\n",
    "    \"bonus=7\\n\"\n",
    "    \"###\\n\"\n",
    ")\n",
    "\n",
    "print(\"prompt len (tokens):\", tokenizer(example_prompt, return_tensors=\"pt\")[\"input_ids\"].shape)\n",
    "print(\"model max_len:\", model.config.max_len)\n",
    "\n",
    "generated = generate_text(model, tokenizer, example_prompt, max_new_tokens=200, device=device)\n",
    "print(\"=== SAMPLE GENERATION ===\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3caa21a",
   "metadata": {},
   "source": [
    "### repetition loop\n",
    "\n",
    "수익률=0.0%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "마지막에 %이 계속해서 반복되는 현상\n",
    "\n",
    "멈춰야하는 신호를 주지않았기 때문에 발생한 문제\n",
    "1. EOS토큰을 stop 조건으로 안씀\n",
    "2. 학습과정에서 eos 토큰을 따로 붙여준 적이 없음\n",
    "\n",
    "-> 문자열 종료에 대한 학습을 시켜 준 적이 없음\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class LottoDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.eos = tokenizer.eos_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_txt = self.texts[idx]\n",
    "\n",
    "        txt = raw_txt + self.eos\n",
    "\n",
    "        # max_len+1 길이로 토큰화 → x:[:-1], y:[1:] 사용\n",
    "        enc = self.tokenizer(\n",
    "            txt,\n",
    "            max_length=self.max_len + 1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = enc[\"input_ids\"][0]        # (max_len+1,)\n",
    "        attn_mask = enc[\"attention_mask\"][0]  # (max_len+1,)\n",
    "\n",
    "        # 언어모델용 input/target\n",
    "        x = ids[:-1]        # (max_len,)\n",
    "        y = ids[1:]         # (max_len,)\n",
    "        x_mask = attn_mask[:-1]\n",
    "\n",
    "        return x, y, x_mask\n",
    "```\n",
    "\n",
    "self.eos = tokenizer.eos_token \n",
    "\n",
    "txt = raw_txt + self.eos\n",
    "\n",
    "해당 부분 추가로 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62ad73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
