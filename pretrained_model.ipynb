{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c66e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned_02 Harry Potter and the Chamber of Secrets.txt 488771 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename):\n",
    "    with open(\"data/\" + filename, 'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "\n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # 줄바꿈을 빈칸으로 변경\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # 여러 빈칸을 하나의 빈칸으로\n",
    "\n",
    "    print(\"data/cleaned_\" + filename, len(cleaned_text), \"characters\") # 글자 수 출력\n",
    "\n",
    "    with open(\"data/cleaned_\" + filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "filenames_list = [\"02 Harry Potter and the Chamber of Secrets.txt\"]\n",
    "\n",
    "for filename in filenames_list:\n",
    "    clean_text(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caf092",
   "metadata": {},
   "source": [
    "##  토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e1dd26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자수 : 72\n",
      "토큰수 : 15\n",
      "[220, 543, 5850, 7342, 6129, 1088, 262, 2119, 24433, 339, 991, 550, 465, 1336, 82]\n",
      "  which Harry watched fly around the room wishing he still had his fulls\n",
      "220 :  \n",
      "543 :  which\n",
      "5850 :  Harry\n",
      "7342 :  watched\n",
      "6129 :  fly\n",
      "1088 :  around\n",
      "262 :  the\n",
      "2119 :  room\n",
      "24433 :  wishing\n",
      "339 :  he\n",
      "991 :  still\n",
      "550 :  had\n",
      "465 :  his\n",
      "1336 :  full\n",
      "82 : s\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"  which Harry watched fly around the room wishing he still had his fulls\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(\"글자수 :\", len(text))  # 글자수: 26\n",
    "print(\"토큰수 :\", len(tokens))  # 토큰수: 6\n",
    "print(tokens)  # [15496, 2159, 257, 281, 3453, 13]\n",
    "print(tokenizer.decode(tokens))  # Harry Potter was a wizard.\n",
    "for token in tokens:\n",
    "    print(token, \":\", tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceac884",
   "metadata": {},
   "source": [
    "### 한글 사용시 해당 토크나이저 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b1582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/ExaOne-3.5-7.8B-Instruct\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# text = \"해리 포터는 마법사였다.\"\n",
    "\n",
    "# tokens = tokenizer.encode(text)\n",
    "\n",
    "# print(\"글자수 :\", len(text))  # 글자수: 14\n",
    "# print(\"토큰수 :\", len(tokens))  # 토큰수: 10\n",
    "# print(tokens)\n",
    "# print(tokenizer.decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "354c8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        print(\"# of tokens in txt:\", len(token_ids))\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b086b",
   "metadata": {},
   "source": [
    "        \n",
    "attention_mask 나중에 참고해야 할듯\n",
    "```python\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b1462b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt: 130520\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/cleaned_02 Harry Potter and the Chamber of Secrets.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# dataset = MyDataset(txt, max_length=512, stride=256)\n",
    "\n",
    "dataset = MyDataset(txt, max_length=32, stride=4)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "# test, valid는 생략\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b8a7698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all we’ve got is a lousy old ghoul in the attic and gnomes all over the garden. House-elves come with big old man\n",
      " we’ve got is a lousy old ghoul in the attic and gnomes all over the garden. House-elves come with big old manors\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0482d7f",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cc56b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "CONTEXT_LENGTH = 128\n",
    "EMB_DIM = 768\n",
    "NUM_HEADS = 12\n",
    "NUM_LAYERS = 12\n",
    "DROP_RATE = 0.1\n",
    "QKV_BIAS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d81ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // NUM_HEADS\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=EMB_DIM,\n",
    "            d_out=EMB_DIM)\n",
    "    \n",
    "        self.ff = FeedForward()\n",
    "        self.norm1 = LayerNorm(EMB_DIM)\n",
    "        self.norm2 = LayerNorm(EMB_DIM)\n",
    "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)])\n",
    "\n",
    "        self.final_norm = LayerNorm(EMB_DIM)\n",
    "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be342a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54746cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens seen: 4096\n",
      "Epoch: 1, Loss: 4.3979428114853505\n",
      "Epoch: 2, Loss: 2.2266207938119185\n",
      "Epoch: 3, Loss: 0.7970686458696531\n",
      "Tokens seen: 4100096\n",
      "Epoch: 4, Loss: 0.39226424870059245\n",
      "Epoch: 5, Loss: 0.304862799489592\n",
      "Epoch: 6, Loss: 0.27067149131316837\n",
      "Epoch: 7, Loss: 0.253944440854816\n",
      "Tokens seen: 8196096\n",
      "Epoch: 8, Loss: 0.2433952234509423\n",
      "Epoch: 9, Loss: 0.2368674221353268\n",
      "Epoch: 10, Loss: 0.23027527848566612\n"
     ]
    }
   ],
   "source": [
    "tokens_seen, global_step = 0, -1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() # Calculate loss gradients\n",
    "        optimizer.step() # Update model weights using loss gradients\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % 1000 == 0:\n",
    "            print(f\"Tokens seen: {tokens_seen}\")\n",
    "        # Optional evaluation step\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_loss}\")\n",
    "    torch.save(model.state_dict(), \"model_\" + str(epoch + 1).zfill(3) + \".pth\")\n",
    "\n",
    "# 주의: 여기서는 편의상 모든 데이터를 train에 사용하였습니다. \n",
    "#      ML에서는 일부 데이터를 validation에 사용하는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "120cda81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "808ac517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(128, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_010.pth\", map_location=device, weights_only=True))\n",
    "model.eval() # dropout X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10c437db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.66\t 257\t  a\n",
      "9.43\t 991\t  still\n",
      "9.04\t 46258\t  valiant\n",
      "8.78\t 284\t  to\n",
      "8.55\t 4978\t  caught\n",
      "8.19\t 30\t ?\n",
      "8.09\t 4762\t  believed\n",
      "8.06\t 10597\t  till\n",
      "8.05\t 1479\t  free\n",
      "7.96\t 262\t  the\n",
      " a\n"
     ]
    }
   ],
   "source": [
    "test = \"Dobby is\"\n",
    "idx = tokenizer.encode(test)\n",
    "idx = torch.tensor(idx).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(idx)\n",
    "\n",
    "logits = logits[:, -1, :]\n",
    "\n",
    "# 가장 확률이 높은 단어 10개 출력\n",
    "top_logits, top_indices = torch.topk(logits, 10) \n",
    "for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n",
    "    print(f\"{p:.2f}\\t {i}\\t {tokenizer.decode([i])}\")\n",
    "\n",
    "# 가장 확률이 높은 단어 출력\n",
    "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "flat = idx_next.squeeze(0) # 배치 차원 제거 torch.Size([1])\n",
    "out = tokenizer.decode(flat.tolist()) # 텐서를 리스트로 바꿔서 디코드\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "213e6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "917c7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Dobby is a house-elf.…” “What does this mean, Albus?” Professor McGonagall asked urgently. “It means finding out Veronica Sm to the whole body sort of Gryffindor dormitwick let out\n",
      "1 : Dobby is a house-elf.…” said Harry. “Well, whoever owns him will be an old wizarding family, and they’ll be ready to be rich,’ll be rich,’ll be rich,�\n",
      "2 : Dobby is a house-elf.…” The voice was growing fainter. Harry was sure it was moving away — moving upward. A mixture of fear spreading up to bewattle cauldron was that he looked dream team how it was stretched out\n",
      "3 : Dobby is a house-elf.…” The voice was growing fainter. Harry was sure it was moving away — moving upward. A mixture of fear spreading up to beaming once more curious people kept waking in the whole body of fear spreading up\n",
      "4 : Dobby is a house-elf.…” said Harry. “Well, whoever owns him will be an old wizarding family, and they’ll be ready to be rich,” said it’ll be rich, as quickly st\n",
      "5 : Dobby is a house-elf.…” The voice was growing fainter. Harry was sure it was moving away — moving upward. A mixture of fear spreading up to beaming once more curious people kept waking up portable, it was moving upward,\n",
      "6 : Dobby is a girls’ toilet.” “Oh, Ron, there won’t be anyone in there,” said Hermione standing up and he said Hermione standing up and then, watching them as they saw the car. “Listen\n",
      "7 : Dobby is a house-elf.…” said Harry. “Well, whoever owns him will be an old wizarding family, and they’ll be ready to be rich,” said cauldron as they” said Lockhart�\n",
      "8 : Dobby is a house-elf.…” said Harry. “Well, whoever owns him will be an old wizarding family, and they’ll be ready to be rich,’ll be rich,’ll beaming my new\n",
      "9 : Dobby is a house-elf.…” said Harry. “Well, whoever owns him will be an old wizarding family, and they’ll be ready to be rich,’ll beheaded,’ll be rich,�\n"
     ]
    }
   ],
   "source": [
    "start_context = input(\"Start context: \")\n",
    "\n",
    "# idx = tokenizer.encode(start_context, allowed_special={'<|endoftext|>'})\n",
    "idx = tokenizer.encode(start_context)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "\n",
    "context_size = model.pos_emb.weight.shape[0] \n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=idx.to(device),\n",
    "        max_new_tokens=50,\n",
    "        context_size= context_size,\n",
    "        top_k=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    out = tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
    "\n",
    "    print(i, \":\", out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
